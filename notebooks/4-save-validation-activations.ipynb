{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import os\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AddGaussianNoise, AddSaltPepperNoise\n",
    "from utils import MagShuffle, PhaseShuffle, AllShuffle\n",
    "from presnet import PResNet18V3NSeparateHP\n",
    "from torchvision.models.resnet import resnet18 as ResNet\n",
    "from torchvision.models import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = 'pnet' #str(sys.argv[1])\n",
    "CKPT_EPOCH = 99 #int(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn_vision_resnet/'\n",
    "dataset_root = '/mnt/smb/locker/abbott-locker/hcnn_vision/imagenet/'\n",
    "ckpt_root = f'{engram_dir}checkpoints/'\n",
    "hps_root = f'{engram_dir}hyperparams/'\n",
    "activations_root = f'{engram_dir}validation_activations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MEAN = [0.485, 0.456, 0.406]\n",
    "TRAIN_STD  = [0.229, 0.224, 0.225]\n",
    "WEIGHT_PATTERN_N = f'{ckpt_root}{TASK_NAME}/'\n",
    "WEIGHT_PATTERN_N += f'pnet_pretrained_pc*_{CKPT_EPOCH:03d}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "    net, weight_pattern, build_graph, random_init,\n",
    "    ff_multiplier, fb_multiplier, er_multiplier, device='cuda:0'\n",
    "    ):\n",
    "\n",
    "    # Initialize PNet\n",
    "    pnet = PResNet18V3NSeparateHP(\n",
    "        net, build_graph=build_graph, random_init=random_init,\n",
    "        ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier,\n",
    "        er_multiplier=er_multiplier, register_backbone_hooks=True)\n",
    "\n",
    "    # Load pcoder weights\n",
    "    for pc in range(pnet.number_of_pcoders):\n",
    "        pc_dict = torch.load(\n",
    "            weight_pattern.replace('*',f'{pc+1}'), map_location='cpu')\n",
    "        pc_dict = pc_dict['pcoderweights']\n",
    "        if 'C_sqrt' not in pc_dict:\n",
    "            pc_dict['C_sqrt'] = torch.tensor(-1, dtype=torch.float)\n",
    "        getattr(pnet, f'pcoder{pc+1}').load_state_dict(pc_dict)\n",
    "\n",
    "    # Set initial hyperparameters\n",
    "    hyperparams = []\n",
    "    for i in range(1, 6):\n",
    "        hps = {}\n",
    "        hps['ffm'] = ff_multiplier\n",
    "        hps['fbm'] = fb_multiplier\n",
    "        hps['erm'] = er_multiplier\n",
    "        hyperparams.append(hps)\n",
    "    pnet.set_hyperparameters(hyperparams)\n",
    "    pnet.eval()\n",
    "    pnet.to(device)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pfile(hps_dir):\n",
    "    best_pfile = None\n",
    "    best_perf = -np.inf\n",
    "    for pfile in os.listdir(hps_dir):\n",
    "        if not pfile.endswith('.p'): continue\n",
    "        with open(f'{hps_dir}{pfile}', 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        for log_idx in range(len(results)-1, 0, -1):\n",
    "            log = results[log_idx]\n",
    "            if ('NoisyPerf' in log[0]) and (log[2]==n_timesteps):\n",
    "                break\n",
    "        mean_perf = np.mean([\n",
    "            log[1] for log in results[log_idx-5:log_idx+1]])\n",
    "        if mean_perf > best_perf:\n",
    "            best_pfile = pfile\n",
    "            best_perf = mean_perf\n",
    "        print(mean_perf)\n",
    "    print(f'best is {best_perf}')\n",
    "    return best_pfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hps_from_pfile(pfile_path):\n",
    "    hps = []\n",
    "    with open(pfile_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    for log_idx in range(len(results)-1, 0, -1):\n",
    "        log = results[log_idx]\n",
    "        if log[0] == 'Hyperparam/pcoder5_memory':\n",
    "            log_idx = log_idx - 19 # Start of hps log\n",
    "            break\n",
    "    for layer in range(1,6):\n",
    "        assert(results[log_idx][0] == f'Hyperparam/pcoder{layer}_feedforward')\n",
    "        layer_hps = {\n",
    "            'ffm': results[log_idx][1],\n",
    "            'fbm': results[log_idx+1][1],\n",
    "            'erm': results[log_idx+2][1]\n",
    "            }\n",
    "        hps.append(layer_hps)\n",
    "        log_idx = log_idx + 4\n",
    "    return hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noises = [\"gaussian_noise\", \"impulse_noise\", \"none\"]\n",
    "noise_gens = [\n",
    "    [AddGaussianNoise(std=0.50),\n",
    "     AddGaussianNoise(std=1.00),\n",
    "     AddGaussianNoise(std=1.50)],\n",
    "    [AddSaltPepperNoise(probability=0.05),\n",
    "     AddSaltPepperNoise(probability=0.15),\n",
    "     AddSaltPepperNoise(probability=0.3)],\n",
    "    [None],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units_per_layer = {\n",
    "    1: (64, 112, 112),\n",
    "    2: (64, 56, 56),\n",
    "    3: (128, 28, 28),\n",
    "    4: (256, 14, 14),\n",
    "    5: (512, 7, 7)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46293333333333336\n",
      "0.46279999999999993\n",
      "0.4606666666666666\n",
      "best is 0.46293333333333336\n",
      "ImageNet Loaded.\n",
      "0.16093333333333334\n",
      "0.1695333333333333\n",
      "0.16640000000000002\n",
      "best is 0.1695333333333333\n",
      "ImageNet Loaded.\n",
      "0.044533333333333334\n",
      "0.04893333333333333\n",
      "0.042333333333333334\n",
      "best is 0.04893333333333333\n",
      "ImageNet Loaded.\n",
      "0.15326666666666666\n",
      "0.15093333333333334\n",
      "0.15313333333333334\n",
      "best is 0.15326666666666666\n",
      "ImageNet Loaded.\n",
      "0.08026666666666667\n",
      "0.07973333333333334\n",
      "0.08086666666666667\n",
      "best is 0.08086666666666667\n",
      "ImageNet Loaded.\n",
      "0.028399999999999998\n",
      "0.026400000000000003\n",
      "0.026866666666666667\n",
      "best is 0.028399999999999998\n",
      "ImageNet Loaded.\n",
      "0.7980666666666667\n",
      "0.7978000000000001\n",
      "0.7978666666666667\n",
      "best is 0.7980666666666667\n",
      "ImageNet Loaded.\n"
     ]
    }
   ],
   "source": [
    "for nt_idx, noise_type in enumerate(all_noises):\n",
    "    for ng_idx, noise_gen in enumerate(noise_gens[nt_idx]):\n",
    "        noise_name = f'{noise_type}_lvl_{ng_idx+1}'\n",
    "        hps_dir = f'{hps_root}{TASK_NAME}/{noise_name}/'\n",
    "        \n",
    "        # Get hps of best-performing iteration\n",
    "        pfile = get_best_pfile(hps_dir)\n",
    "        hps = get_hps_from_pfile(f'{hps_dir}{pfile}')\n",
    "        \n",
    "        # Set network\n",
    "        net = ResNet(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        pnet = load_pnet(\n",
    "            net, WEIGHT_PATTERN_N,\n",
    "            build_graph=True, random_init=False, ff_multiplier=0.33,\n",
    "            fb_multiplier=0.33, er_multiplier=0.)\n",
    "        pnet.set_hyperparameters(hps)\n",
    "            \n",
    "        # Set up transforms\n",
    "        transform_seq = [\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=TRAIN_MEAN, std=TRAIN_STD),]\n",
    "        if noise_gen is not None:\n",
    "            transform_seq.append(noise_gen)\n",
    "        transform_seq = transforms.Compose(transform_seq)\n",
    "        \n",
    "        # If activations already calculated, skip\n",
    "        activations_dir = f'{activations_root}{TASK_NAME}/{noise_name}/'\n",
    "        os.makedirs(activations_dir, exist_ok=True)\n",
    "        hdf5_path = f'{activations_dir}{pfile[:-2]}.hdf5'\n",
    "        if os.path.exists(hdf5_path): continue\n",
    "        \n",
    "        # Load dataset\n",
    "        np.random.seed(1)\n",
    "        val_subset_indices = np.random.choice(50000, size=800, replace=False)\n",
    "        np.random.seed()\n",
    "        val_ds = ImageNet(dataset_root, split='val', transform=transform_seq)\n",
    "        val_subset = torch.utils.data.Subset(val_ds, val_subset_indices)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=1, drop_last=False)\n",
    "        del val_ds\n",
    "        print('ImageNet Loaded.')\n",
    "        \n",
    "        # Run and save\n",
    "        with h5py.File(hdf5_path, 'x') as f_out:\n",
    "            # Initialize hdf5 containers\n",
    "            data_dict = {}\n",
    "            for layer in np.arange(1, 6):\n",
    "                activ_dim = (len(val_loader),) + n_units_per_layer[layer]\n",
    "                for timestep in range(n_timesteps):\n",
    "                    data_dict[f'{layer}_{timestep}_activations'] = f_out.create_dataset(\n",
    "                        f'{layer}_{timestep}_activations', activ_dim, dtype='float32'\n",
    "                        )\n",
    "            data_dict['labels'] = f_out.create_dataset('labels', len(val_loader), dtype='int')\n",
    "            for timestep in range(5):\n",
    "                data_dict[f'label_{timestep}'] = f_out.create_dataset(\n",
    "                    f'label_{timestep}', len(val_loader), dtype='int')\n",
    "                \n",
    "            # Feed inputs into network\n",
    "            for d_idx, (_in, _label) in enumerate(val_loader):\n",
    "                pnet.reset()\n",
    "                _in = _in.to(DEVICE)\n",
    "                data_dict['labels'][d_idx] = _label.item()\n",
    "                for t in range(n_timesteps): \n",
    "                    _in_t = _in if t == 0 else None\n",
    "                    with torch.no_grad():\n",
    "                        output = pnet(_in_t)\n",
    "                    pred_label = output.max(-1)[1].item()\n",
    "                    for layer in np.arange(1,6):\n",
    "                        data_dict[f'{layer}_{t}_activations'][d_idx] = getattr(\n",
    "                            pnet, f'block{layer}_repr').detach().cpu().numpy()\n",
    "                    data_dict[f'label_{t}'][d_idx] = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hcnn]",
   "language": "python",
   "name": "conda-env-.conda-hcnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

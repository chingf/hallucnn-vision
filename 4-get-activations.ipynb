{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AddGaussianNoise, AddSaltPepperNoise\n",
    "from utils import MagShuffle, PhaseShuffle, AllShuffle\n",
    "from timm.models import efficientnet_b0\n",
    "from peff_b0 import PEffN_b0SeparateHP_V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = 'pnet' #str(sys.argv[1])\n",
    "CKPT_EPOCH = 49 #int(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "engram_dir = '/mnt/smb/locker/abbott-locker/hcnn_vision/'\n",
    "dataset_root = f'{engram_dir}imagenet/'\n",
    "ckpt_root = f'{engram_dir}checkpoints/'\n",
    "hps_root = f'{engram_dir}hyperparams/'\n",
    "activations_root = f'{engram_dir}activations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MEAN = [0.485, 0.456, 0.406]\n",
    "TRAIN_STD  = [0.229, 0.224, 0.225]\n",
    "WEIGHT_PATTERN_N = f'{ckpt_root}{TASK_NAME}/'\n",
    "WEIGHT_PATTERN_N += f'pnet_pretrained_pc*_{CKPT_EPOCH:03d}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pnet(\n",
    "    net, weight_pattern, build_graph, random_init,\n",
    "    ff_multiplier, fb_multiplier, er_multiplier, same_param, device='cuda:0'\n",
    "    ):\n",
    "\n",
    "    if same_param:\n",
    "        raise Exception('Not implemented!')\n",
    "    else:\n",
    "        pnet = PEffN_b0SeparateHP_V1(\n",
    "            net, build_graph=build_graph, random_init=random_init,\n",
    "            ff_multiplier=ff_multiplier, fb_multiplier=fb_multiplier, er_multiplier=er_multiplier,\n",
    "            register_backbone_hooks=True)\n",
    "\n",
    "    for pc in range(pnet.number_of_pcoders):\n",
    "        pc_dict = torch.load(weight_pattern.replace('*',f'{pc+1}'), map_location='cpu')\n",
    "        pc_dict = pc_dict['pcoderweights']\n",
    "        if 'C_sqrt' not in pc_dict:\n",
    "            pc_dict['C_sqrt'] = torch.tensor(-1, dtype=torch.float)\n",
    "        getattr(pnet, f'pcoder{pc+1}').load_state_dict(pc_dict)\n",
    "\n",
    "    pnet.eval()\n",
    "    pnet.to(DEVICE)\n",
    "    return pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pfile(hps_dir):\n",
    "    best_pfile = None\n",
    "    best_perf = -np.inf\n",
    "    for pfile in os.listdir(hps_dir):\n",
    "        if not pfile.endswith('.p'): continue\n",
    "        with open(f'{hps_dir}{pfile}', 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        for log_idx in range(len(results)-1, 0, -1):\n",
    "            log = results[log_idx]\n",
    "            if ('NoisyPerf' in log[0]) and (log[2]==5):\n",
    "                break\n",
    "        mean_perf = np.mean([\n",
    "            log[1] for log in results[log_idx-5:log_idx+1]])\n",
    "        if mean_perf > best_perf:\n",
    "            best_pfile = pfile\n",
    "            best_perf = mean_perf\n",
    "        print(mean_perf)\n",
    "    print(f'best is {best_perf}')\n",
    "    return best_pfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hps_from_pfile(pfile_path):\n",
    "    hps = []\n",
    "    with open(pfile_path, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    for log_idx in range(len(results)-1, 0, -1):\n",
    "        log = results[log_idx]\n",
    "        if log[0] == 'Hyperparam/pcoder8_memory':\n",
    "            log_idx = log_idx - 31 # Start of hps log\n",
    "            break\n",
    "    for layer in range(1,9):\n",
    "        assert(results[log_idx][0] == f'Hyperparam/pcoder{layer}_feedforward')\n",
    "        layer_hps = {\n",
    "            'ffm': results[log_idx][1],\n",
    "            'fbm': results[log_idx+1][1],\n",
    "            'erm': results[log_idx+2][1]\n",
    "            }\n",
    "        hps.append(layer_hps)\n",
    "        log_idx = log_idx + 4\n",
    "    return hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noises = [\"gaussian_noise\", \"impulse_noise\", \"none\"]\n",
    "noise_gens = [\n",
    "    [AddGaussianNoise(std=0.50),\n",
    "     AddGaussianNoise(std=1.00),\n",
    "     AddGaussianNoise(std=1.50)],\n",
    "    [AddSaltPepperNoise(probability=0.05),\n",
    "     AddSaltPepperNoise(probability=0.15),\n",
    "     AddSaltPepperNoise(probability=0.3)],\n",
    "    [None],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units_per_layer = {\n",
    "    1: (32, 112, 112),\n",
    "    2: (16, 112, 112),\n",
    "    3: (24, 56, 56),\n",
    "    4: (40, 28, 28),\n",
    "    5: (80, 14, 14),\n",
    "    6: (112, 14, 14),\n",
    "    7: (192, 7, 7),\n",
    "    8: (320, 7, 7)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6685166666666666\n",
      "0.6659777777777778\n",
      "0.59646\n",
      "0.666561111111111\n",
      "best is 0.6685166666666666\n",
      "ImageNet Loaded.\n"
     ]
    }
   ],
   "source": [
    "for nt_idx, noise_type in enumerate(all_noises):\n",
    "    for ng_idx, noise_gen in enumerate(noise_gens[nt_idx]):\n",
    "        if noise_type == 'none': ng_idx = -1\n",
    "        noise_name = f'{noise_type}_lvl_{ng_idx+1}'\n",
    "        hps_dir = f'{hps_root}{TASK_NAME}/{noise_name}/'\n",
    "        \n",
    "        # Get hps of best-performing iteration\n",
    "        pfile = get_best_pfile(hps_dir)\n",
    "        hps = get_hps_from_pfile(f'{hps_dir}{pfile}')\n",
    "        \n",
    "        # Set network\n",
    "        net = efficientnet_b0(pretrained=True)\n",
    "        pnet = load_pnet(\n",
    "            net, WEIGHT_PATTERN_N,\n",
    "            build_graph=True, random_init=False, ff_multiplier=0.33,\n",
    "            fb_multiplier=0.33, er_multiplier=0.0, same_param=False)\n",
    "        pnet.set_hyperparameters(hps)\n",
    "            \n",
    "        # Set up transforms\n",
    "        transform_seq = [\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=TRAIN_MEAN, std=TRAIN_STD),]\n",
    "        if noise_gen is not None:\n",
    "            transform_seq.append(noise_gen)\n",
    "        transform_seq = transforms.Compose(transform_seq)\n",
    "        \n",
    "        # Load dataset\n",
    "        np.random.seed(1)\n",
    "        val_subset_indices = np.random.choice(50000, size=600, replace=False)\n",
    "        np.random.seed()\n",
    "        val_ds = ImageNet(dataset_root, split='val', transform=transform_seq)\n",
    "        val_subset = torch.utils.data.Subset(val_ds, val_subset_indices)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=1, drop_last=False)\n",
    "        del val_ds\n",
    "        print('ImageNet Loaded.')\n",
    "        \n",
    "        # Run and save\n",
    "        activations_dir = f'{activations_root}{TASK_NAME}/{noise_name}/'\n",
    "        os.makedirs(activations_dir, exist_ok=True)\n",
    "        hdf5_path = f'{activations_dir}{pfile[:-2]}.hdf5'\n",
    "        with h5py.File(hdf5_path, 'x') as f_out:\n",
    "            # Initialize hdf5 containers\n",
    "            data_dict = {}\n",
    "            for layer in np.arange(1, 9):\n",
    "                activ_dim = (len(val_loader),) + n_units_per_layer[layer]\n",
    "                for timestep in range(5):\n",
    "                    data_dict[f'{layer}_{timestep}_activations'] = f_out.create_dataset(\n",
    "                        f'{layer}_{timestep}_activations', activ_dim, dtype='float32'\n",
    "                        )\n",
    "            data_dict['labels'] = f_out.create_dataset(\n",
    "                'labels', len(val_loader), dtype='int')\n",
    "            for timestep in range(5):\n",
    "                data_dict[f'label_{timestep}'] = f_out.create_dataset(\n",
    "                    f'label_{timestep}', len(val_loader), dtype='int')\n",
    "                \n",
    "            # Feed inputs into network\n",
    "            for d_idx, (_in, _label) in enumerate(val_loader):\n",
    "                pnet.reset()\n",
    "                _in = _in.to(DEVICE)\n",
    "                data_dict['labels'][d_idx] = _label.item()\n",
    "                for t in range(n_timesteps): \n",
    "                    _in_t = _in if t == 0 else None\n",
    "                    with torch.no_grad():\n",
    "                        output = pnet(_in_t)\n",
    "                    pred_label = output.max(-1)[1].item()\n",
    "                    for layer in np.arange(1,9):\n",
    "                        data_dict[f'{layer}_{t}_activations'][d_idx] = getattr(\n",
    "                            pnet, f'block{layer}_repr').detach().cpu().numpy()\n",
    "                    data_dict[f'label_{timestep}'][d_idx] = pred_label\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hcnn]",
   "language": "python",
   "name": "conda-env-.conda-hcnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
